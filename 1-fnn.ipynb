{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model: Neural Language Models (Bengio et al. 2003) URL: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "Dimension key:\n",
    "# windows\n",
    "B: batch size\n",
    "T: sequence length\n",
    "\n",
    "# input/output\n",
    "V: vocabulary size\n",
    "E: embedding dimension\n",
    "D: model dimension\n",
    "\"\"\"\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# from jaxtyping import ...\n",
    "g = torch.Generator().manual_seed(1337) # for .randn()\n",
    "\n",
    "B, T = 32, 3\n",
    "V, E, D = 27, 10, 200\n",
    "\n",
    "# step: 0/200000, loss 27.63208770751953\n",
    "# -> expected loss = nll = p(c) = -torch.tensor(1/V=27).log() = 3.2958\n",
    "# -> self.W = torch.randn() is sampling from N(0, 1)\n",
    "# -> just initialize weights with gain/sqrt(D_in) or torch.init_kaimingnormal()\n",
    "\n",
    "# residuals + normalization + Adam/RMSprop has made initialization less fragile\n",
    "# -> b/c initialization is fragile/intractable with *deep* neural networks\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, D_in, D_out, bias=True):\n",
    "        self.W_DiDo = torch.randn((D_in, D_out), generator=g) * (5/3)/D_in**0.5 # kaiming init (He et al. 2015)\n",
    "        self.b_Do = torch.zeros(D_out) if bias else None\n",
    "\n",
    "    def __call__(self, X_Di):\n",
    "        self.X_Do = X_Di @ self.W_DiDo\n",
    "        if self.b_Do is not None:\n",
    "            self.X_Do += self.b_Do\n",
    "        # hidden layer:                 h(x; W) := W_D(l-1)D(l) @ X\n",
    "        # hidden layer + batchorm    h(x, b; W) := W_D(l-1)D(l) @ batch_norm(X_b)\n",
    "        #                batchnorm(X_b) = batch_gamma + (X_batch-mew)/sigma + batch_beta (Ioffe, Szegedy 2015)\n",
    "        \n",
    "        # -> batchnorm performs activation-distribution-agnostic normalization by normalizing initial weights to unit gaussian\n",
    "        # -> fragility of kaiming initialization wrt different *activation functions* and *deeper networks* isn't as crucical\n",
    "            # -> random batch is effectively regularizing by introducing some entropy (jitter) in the distribution of activations\n",
    "            # -> alternatives to batchnorm have been proposed to decouple h as a function from the batch: layernorm, instancenorm, groupnorm\n",
    "        self.X_Do = b_gamma + (self.X_Do - self.X_Do.mean(0, keepdim=True))/self.X_Do.std(0, keepdim=True) + b_beta # batchnorm: .mean() and .std() are differentiable.\n",
    "        return self.X_Do\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.W_DiDo] + ([] if self.b_Do is None else [self.b_Do])\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, X_BD):\n",
    "        self.X_BD = torch.tanh(X_BD)\n",
    "        # plt.hist(self.X_BD.view(-1).tolist(), 50); # distribution of weights\n",
    "        # plt.imshow(self.X_BD.abs() > 0.99, cmap='gray', interpolation='nearest') # vanishing gradients\n",
    "        self.out = self.X_BD\n",
    "        return self.X_BD\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "model = [\n",
    "    Linear(T * E, D), Tanh(),\n",
    "    Linear(D, D), Tanh(),\n",
    "    Linear(D, V),\n",
    "]\n",
    "\n",
    "C = torch.randn((V,E), generator=g)\n",
    "params = [C] + [p for l in model for p in l.parameters()]\n",
    "for p in params:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mooose torch.Size([1, 200])\n",
      "mooose torch.Size([1, 200])\n",
      "mooose torch.Size([1, 27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10ff348c0>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. dataloader\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "words = open('./data/names.txt', 'r').read().splitlines()\n",
    "v = sorted(list(set(''.join(words))))\n",
    "encode = { c:i+1 for i,c in enumerate(v) }\n",
    "encode['.'] = 0\n",
    "decode = { i:c for c,i in encode.items() }\n",
    "\n",
    "context_length = 3\n",
    "def gen_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * context_length;\n",
    "        for c in w + '.':\n",
    "            X.append(context)\n",
    "            Y.append(encode[c])\n",
    "            # print(''.join(decode[i] for i in context), '-->', decode[encode[c]])\n",
    "            context = context[1:] + [encode[c]]\n",
    "    X, Y = torch.tensor(X), torch.tensor(Y) # X:(N,C) Y:(N)\n",
    "    return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1, n2 = int(0.8*len(words)), int(0.9*len(words))\n",
    "Xtr, Ytr = gen_dataset(words[:n1])\n",
    "Xdev, Ydev = gen_dataset(words[n1:n2])\n",
    "Xte, Yte = gen_dataset(words[n2:])\n",
    "\n",
    "# 2. training loop\n",
    "N = Xtr.shape[0]\n",
    "losses, steps = [], []\n",
    "for step in range(200000):\n",
    "    # 1. forward\n",
    "    indices_B = torch.randint(0, N, (B,))\n",
    "    X_B, Y_B = Xtr[indices_B], Ytr[indices_B]\n",
    "\n",
    "    X_BD = C[X_B].view(-1, T * E)\n",
    "    for layer in model:\n",
    "        X_BD = layer(X_BD)\n",
    "    loss = F.cross_entropy(X_BD, Y_B)\n",
    "\n",
    "    # 2. backward\n",
    "    for layer in model:\n",
    "        layer.out.retain_grad()\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # 3. update\n",
    "    for p in params:\n",
    "        p.data += -0.01 * p.grad\n",
    "\n",
    "    steps.append(step)\n",
    "    losses.append(loss.log10().item())\n",
    "    break\n",
    "    if step % 10000 == 0:\n",
    "        print(f\"step: {step}/{200000}, loss {loss.item()}\")\n",
    "\n",
    "plt.plot(steps, losses)\n",
    "\n",
    "# generalization\n",
    "# @torch.no_grad() # this decorator disables gradient tracking\n",
    "# def split_loss(split):\n",
    "#   x,y = {\n",
    "#     'train': (Xtr, Ytr),\n",
    "#     'val': (Xdev, Ydev),\n",
    "#     'test': (Xte, Yte),\n",
    "#   }[split]\n",
    "#   emb = C[x] # (N, block_size, n_embd)\n",
    "#   x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "#   for layer in layers:\n",
    "#     x = layer(x)\n",
    "#   loss = F.cross_entropy(x, y)\n",
    "#   print(split, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inference loop\n",
    "# i_terminal = 0\n",
    "# for _ in range(20):\n",
    "#     output = []\n",
    "#     context = [0] * context_length\n",
    "#     while True:\n",
    "#         emb = C[torch.tensor([context])]\n",
    "#         x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "#         for l in model:\n",
    "#           x = l(x)\n",
    "#         logits = x\n",
    "#         y_hat = F.softmax(logits, dim=1)\n",
    "\n",
    "#         i = torch.multinomial(y_hat, num_samples=1, replacement=True, generator=g).item()\n",
    "#         context = context[1:] + [i]\n",
    "#         output.append(decode[i])\n",
    "#         if i == i_terminal:\n",
    "#             break\n",
    "#     print(''.join(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
