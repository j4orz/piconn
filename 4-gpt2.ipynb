{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "model loaded to cuda\n"
     ]
    }
   ],
   "source": [
    "\"\"\" model: gpt2\n",
    "- (Vaswani et al. 2017 https://arxiv.org/abs/1706.03762)\n",
    "- (Radford et al. 2019 https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "- (Brown et al. 2020 https://arxiv.org/abs/2005.14165)\n",
    "\n",
    "pre-gpt       ->    gpt2                                 URL\n",
    "-----------------------------------------------------------------------------------------\n",
    "- ReLU        ->    GeLU: (Hendrycks, Gimpel 2016)       https://arxiv.org/abs/1606.08415\n",
    "- BatchNorm   ->    LayerNorm: (Ba et al. 2016)          https://arxiv.org/abs/1607.06450\n",
    "- N/A         ->    Residuals: (He et al. 2015)          https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "\n",
    "Dimension key:\n",
    "\n",
    "# windows\n",
    "B: batch size\n",
    "T: sequence length\n",
    "\n",
    "# input/output\n",
    "V: vocabulary size\n",
    "D: model dimension (n_embd)\n",
    "\n",
    "# attention\n",
    "N: number of transformer blocks (n_layer)\n",
    "H: number of attention heads in a layer (n_head)\n",
    "K: size of each attention key or value (n_k)\n",
    "\"\"\"\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    # windows: B, T\n",
    "    batch_size: int = -1   # B\n",
    "    block_size: int = 1024  # T\n",
    "    # input/output:  V, D\n",
    "    vocab_size: int = 50257  # V (256 bytes + 50,000 BPE merges + 1 <|endoftext|> token)\n",
    "    n_embd: int = 768      # D\n",
    "    # attn: NH\n",
    "    n_layer: int = 12      # N\n",
    "    n_head: int = 12       # H\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        T, D, H = config.block_size, config.n_embd, config.n_head\n",
    "        assert D % H == 0\n",
    "\n",
    "        self.H = H\n",
    "\n",
    "        self.c_attn = nn.Linear(D, 3 * D)\n",
    "        self.c_proj = nn.Linear(D, D)\n",
    "        self.register_buffer('bias', torch.tril(torch.ones(T, T)).view(1, 1, T, T)) # tril -> bias for HF\n",
    "\n",
    "    def forward(self, X_BTD):\n",
    "        B,T,D = X_BTD.shape\n",
    "        H = self.H\n",
    "        # 1. project to learned QKV subspaces Q=WqX, K=WkX, V=WvX\n",
    "        Wq_DK, Wk_DK, Wv_DK = self.c_attn(X_BTD).split(D, dim=2)\n",
    "        Q_BHTK, K_BHTK, V_BHTK = Wq_DK.view(B, T, H, D // H).transpose(1, 2), Wk_DK.view(B, T, H, D // H).transpose(1, 2), Wv_DK.view(B, T, H, D // H).transpose(1, 2)\n",
    "\n",
    "        # 2. evaluate scores A(QKV) = softmax(QK^T/sqrt(d_k))V\n",
    "        A_BHTT = Q_BHTK @ K_BHTK.transpose(-2, -1) * (1.0 / math.sqrt(K_BHTK.size(-1)))\n",
    "        A_BHTT = A_BHTT.masked_fill(self.bias[:, :, :T, :T]==0, float('-inf'))\n",
    "        A_BHTT = F.softmax(A_BHTT, dim=-1) # todo, when dim=-1?\n",
    "\n",
    "        # 3. contextualize the embeddings\n",
    "        S_BHTD = A_BHTT @ V_BHTK\n",
    "        S_BTD = S_BHTD.transpose(1, 2).contiguous().view(B, T, D) # performs cat\n",
    "        S_BTD = self.c_proj(S_BTD)\n",
    "\n",
    "        return S_BTD\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        D = config.n_embd\n",
    "        self.c_fc = nn.Linear(D, 4*D) # projecting up to extract features from context embeddings\n",
    "        self.gelu = nn.GELU(approximate='tanh') # (Hendrycks et al. https://arxiv.org/abs/1606.08415)\n",
    "        self.c_proj = nn.Linear(4*D, D) # projecting back down to residual pathway\n",
    "\n",
    "    def forward(self, X_BTD):\n",
    "        X_BT4D = self.c_fc(X_BTD)\n",
    "        X_BT4D = self.gelu(X_BT4D)\n",
    "        X_BTD = self.c_proj(X_BT4D)\n",
    "        return X_BTD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class LayerNorm(nn.Module): # manual inefficient LayerNorm implementation\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward():\n",
    "#         # ...\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        D, H = config.n_embd, config.n_head\n",
    "        self.ln_1 = nn.LayerNorm(D)\n",
    "        self.attn = MHA(config)\n",
    "        self.mlp = FFN(config) # .mlp for HF\n",
    "        self.ln_2 = nn.LayerNorm(D)\n",
    "\n",
    "    def forward(self, X_BTD):\n",
    "        # residuals:\n",
    "        # - (He et al. 2015 https://arxiv.org/abs/1512.03385)\n",
    "        # - (Elhage et al. 2021 https://transformer-circuits.pub/2021/framework/index.html)\n",
    "        X_BTD = X_BTD + self.attn(self.ln_1(X_BTD))\n",
    "        X_BTD = X_BTD + self.mlp(self.ln_2(X_BTD))\n",
    "        return X_BTD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        B, T = config.batch_size, config.block_size\n",
    "        V, D = config.vocab_size, config.n_embd\n",
    "        N, H = config.n_layer, config.n_head\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(V, D), # Wt\n",
    "            wpe = nn.Embedding(T, D), # Wp\n",
    "            h = nn.ModuleList([Block(config) for _ in range(N)]),\n",
    "            ln_f = nn.LayerNorm(D),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(D, V, bias=False)\n",
    "\n",
    "    def forward(self, X_BT, Y_BT=None): # Some(Y_BT) => training, None => inference\n",
    "        B, T = X_BT.shape\n",
    "        # 1. embedding: BTD\n",
    "        Xtok_BTD = self.transformer.wte(X_BT)\n",
    "        Xpos_TD = self.transformer.wpe(torch.arange(0, T, dtype=torch.long, device=X_BT.device))\n",
    "        X_BTD = Xtok_BTD + Xpos_TD\n",
    "        # 2. N transformer blocks: Nx(BTD -> BTK -> BTD)\n",
    "        for h in self.transformer.h:\n",
    "            X_BTD = h(X_BTD)\n",
    "        # 3. logits: BTD -> BTV\n",
    "        X_BTD = self.transformer.ln_f(X_BTD)\n",
    "        logits_BTV = self.lm_head(X_BTD)\n",
    "        loss = None\n",
    "\n",
    "        if Y_BT is not None:\n",
    "            V = self.config.vocab_size\n",
    "            loss = F.cross_entropy(logits_BTV.view(B*T, V), Y_BT.view(B*T)) # reshape for .cross_entropy()\n",
    "        return logits_BTV, loss\n",
    " \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "\n",
    "\n",
    "\n",
    "        # 1. model init\n",
    "        model_hf, model = GPT2LMHeadModel.from_pretrained(model_type), GPT(GPTConfig(**config_args))\n",
    "        sdhf, sd = model_hf.state_dict(), model.state_dict()\n",
    "        sdhf_keys, sd_keys = sdhf.keys(), sd.keys() # .collect::<Vec<_>>() semantics\n",
    "        # filter\n",
    "        sdhf_keys = [k for k in sdhf_keys if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sdhf_keys = [k for k in sdhf_keys if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # 2. copy\n",
    "        # ensure all params match in name and shape\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        assert len(sdhf_keys) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sdhf_keys:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sdhf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sdhf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sdhf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sdhf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "# model = GPT.from_pretrained('gpt2')\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "print(f'model loaded to {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5962, 22307,    25,   198,  8421,   356,  5120,   597],\n",
      "        [ 2252,    11,  3285,   502,  2740,    13,   198,   198],\n",
      "        [ 3237,    25,   198,  5248,   461,    11,  2740,    13],\n",
      "        [  198,   198,  5962, 22307,    25,   198,  1639,   389]],\n",
      "       device='cuda:0')\n",
      "tensor([[22307,    25,   198,  8421,   356,  5120,   597,  2252],\n",
      "        [   11,  3285,   502,  2740,    13,   198,   198,  3237],\n",
      "        [   25,   198,  5248,   461,    11,  2740,    13,   198],\n",
      "        [  198,  5962, 22307,    25,   198,  1639,   389,   477]],\n",
      "       device='cuda:0')\n",
      "batch 0\n",
      "x: tensor([5962], device='cuda:0') -> y: tensor(22307, device='cuda:0')\n",
      "x: tensor([ 5962, 22307], device='cuda:0') -> y: tensor(25, device='cuda:0')\n",
      "x: tensor([ 5962, 22307,    25], device='cuda:0') -> y: tensor(198, device='cuda:0')\n",
      "x: tensor([ 5962, 22307,    25,   198], device='cuda:0') -> y: tensor(8421, device='cuda:0')\n",
      "x: tensor([ 5962, 22307,    25,   198,  8421], device='cuda:0') -> y: tensor(356, device='cuda:0')\n",
      "x: tensor([ 5962, 22307,    25,   198,  8421,   356], device='cuda:0') -> y: tensor(5120, device='cuda:0')\n",
      "x: tensor([ 5962, 22307,    25,   198,  8421,   356,  5120], device='cuda:0') -> y: tensor(597, device='cuda:0')\n",
      "x: tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597],\n",
      "       device='cuda:0') -> y: tensor(2252, device='cuda:0')\n",
      "batch 1\n",
      "x: tensor([2252], device='cuda:0') -> y: tensor(11, device='cuda:0')\n",
      "x: tensor([2252,   11], device='cuda:0') -> y: tensor(3285, device='cuda:0')\n",
      "x: tensor([2252,   11, 3285], device='cuda:0') -> y: tensor(502, device='cuda:0')\n",
      "x: tensor([2252,   11, 3285,  502], device='cuda:0') -> y: tensor(2740, device='cuda:0')\n",
      "x: tensor([2252,   11, 3285,  502, 2740], device='cuda:0') -> y: tensor(13, device='cuda:0')\n",
      "x: tensor([2252,   11, 3285,  502, 2740,   13], device='cuda:0') -> y: tensor(198, device='cuda:0')\n",
      "x: tensor([2252,   11, 3285,  502, 2740,   13,  198], device='cuda:0') -> y: tensor(198, device='cuda:0')\n",
      "x: tensor([2252,   11, 3285,  502, 2740,   13,  198,  198], device='cuda:0') -> y: tensor(3237, device='cuda:0')\n",
      "batch 2\n",
      "x: tensor([3237], device='cuda:0') -> y: tensor(25, device='cuda:0')\n",
      "x: tensor([3237,   25], device='cuda:0') -> y: tensor(198, device='cuda:0')\n",
      "x: tensor([3237,   25,  198], device='cuda:0') -> y: tensor(5248, device='cuda:0')\n",
      "x: tensor([3237,   25,  198, 5248], device='cuda:0') -> y: tensor(461, device='cuda:0')\n",
      "x: tensor([3237,   25,  198, 5248,  461], device='cuda:0') -> y: tensor(11, device='cuda:0')\n",
      "x: tensor([3237,   25,  198, 5248,  461,   11], device='cuda:0') -> y: tensor(2740, device='cuda:0')\n",
      "x: tensor([3237,   25,  198, 5248,  461,   11, 2740], device='cuda:0') -> y: tensor(13, device='cuda:0')\n",
      "x: tensor([3237,   25,  198, 5248,  461,   11, 2740,   13], device='cuda:0') -> y: tensor(198, device='cuda:0')\n",
      "batch 3\n",
      "x: tensor([198], device='cuda:0') -> y: tensor(198, device='cuda:0')\n",
      "x: tensor([198, 198], device='cuda:0') -> y: tensor(5962, device='cuda:0')\n",
      "x: tensor([ 198,  198, 5962], device='cuda:0') -> y: tensor(22307, device='cuda:0')\n",
      "x: tensor([  198,   198,  5962, 22307], device='cuda:0') -> y: tensor(25, device='cuda:0')\n",
      "x: tensor([  198,   198,  5962, 22307,    25], device='cuda:0') -> y: tensor(198, device='cuda:0')\n",
      "x: tensor([  198,   198,  5962, 22307,    25,   198], device='cuda:0') -> y: tensor(1639, device='cuda:0')\n",
      "x: tensor([  198,   198,  5962, 22307,    25,   198,  1639], device='cuda:0') -> y: tensor(389, device='cuda:0')\n",
      "x: tensor([  198,   198,  5962, 22307,    25,   198,  1639,   389],\n",
      "       device='cuda:0') -> y: tensor(477, device='cuda:0')\n",
      "==========================================\n",
      "step: 0, loss: 11.003739356994629\n",
      "step: 1, loss: 4.528020858764648\n",
      "step: 2, loss: 1.6381187438964844\n",
      "step: 3, loss: 0.5263156890869141\n",
      "step: 4, loss: 0.18431271612644196\n",
      "step: 5, loss: 0.07252059131860733\n",
      "step: 6, loss: 0.04728636518120766\n",
      "step: 7, loss: 0.038431111723184586\n",
      "step: 8, loss: 0.031630247831344604\n",
      "step: 9, loss: 0.024728871881961823\n",
      "step: 10, loss: 0.019038289785385132\n",
      "step: 11, loss: 0.015147249214351177\n",
      "step: 12, loss: 0.012712202966213226\n",
      "step: 13, loss: 0.011214916594326496\n",
      "step: 14, loss: 0.010254690423607826\n",
      "step: 15, loss: 0.009573202580213547\n",
      "step: 16, loss: 0.009006683714687824\n",
      "step: 17, loss: 0.008445958606898785\n",
      "step: 18, loss: 0.007838345132768154\n",
      "step: 19, loss: 0.00719471275806427\n",
      "step: 20, loss: 0.00656130351126194\n",
      "step: 21, loss: 0.0059816292487084866\n",
      "step: 22, loss: 0.005479727406054735\n",
      "step: 23, loss: 0.005061271134763956\n",
      "step: 24, loss: 0.004720473662018776\n",
      "step: 25, loss: 0.004445751663297415\n",
      "step: 26, loss: 0.004223370924592018\n",
      "step: 27, loss: 0.004039858467876911\n",
      "step: 28, loss: 0.0038831287529319525\n",
      "step: 29, loss: 0.0037435770500451326\n",
      "step: 30, loss: 0.003614514833316207\n",
      "step: 31, loss: 0.0034920976031571627\n",
      "step: 32, loss: 0.0033744422253221273\n",
      "step: 33, loss: 0.0032611556816846132\n",
      "step: 34, loss: 0.00315282610245049\n",
      "step: 35, loss: 0.003049928229302168\n",
      "step: 36, loss: 0.0029529931489378214\n",
      "step: 37, loss: 0.0028622790705412626\n",
      "step: 38, loss: 0.0027778393123298883\n",
      "step: 39, loss: 0.002699790056794882\n",
      "step: 40, loss: 0.0026277778670191765\n",
      "step: 41, loss: 0.0025615061167627573\n",
      "step: 42, loss: 0.00250053103081882\n",
      "step: 43, loss: 0.002444549696519971\n",
      "step: 44, loss: 0.002393078524619341\n",
      "step: 45, loss: 0.002345690503716469\n",
      "step: 46, loss: 0.0023019732907414436\n",
      "step: 47, loss: 0.0022615203633904457\n",
      "step: 48, loss: 0.002223896561190486\n",
      "step: 49, loss: 0.00218873075209558\n",
      "0.00218873075209558\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAGdCAYAAADaPpOnAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPF1JREFUeJzt3Xl8VPW9x//3TJKZ7AnZExJCWIPsBIGgrVjyE+ymt9Rqq0WtYqXYX13ubeHeVmuXH9auV2uL1Spy3W3V1qVYBMEFZI8sQiRsCdkXMpNMlkky5/dHksHIGsjkzExez8fjPDI5S87H88iDvP2e72IxDMMQAABAgLCaXQAAAEBfEF4AAEBAIbwAAICAQngBAAABhfACAAACCuEFAAAEFMILAAAIKIQXAAAQUELNLqC/eTwelZeXKyYmRhaLxexyAADAOTAMQ42NjcrIyJDVeua2laALL+Xl5crKyjK7DAAAcB5KS0uVmZl5xnOCLrzExMRI6vqPj42NNbkaAABwLpxOp7Kysrx/x88k6MJLz6ui2NhYwgsAAAHmXLp80GEXAAAEFMILAAAIKIQXAAAQUAgvAAAgoBBeAABAQCG8AACAgEJ4AQAAAYXwAgAAAgrhBQAABBTCCwAACCiEFwAAEFAILwAAIKAE3cKMvlLe0KIXtpaqtaNTy64cZ3Y5AAAMWj5teXn33Xf1la98RRkZGbJYLHr11VfPes369es1bdo02e12jRo1SitXrvRliefM0dKu/117QKs2HpW7w2N2OQAADFo+DS8ul0uTJ0/WI488ck7nHz58WF/60pd0+eWXq7CwUHfeeaduvfVWvfXWW74s85yMTY1RYpRNLe2dKixtMLscAAAGLZ++Nrryyit15ZVXnvP5K1asUE5Ojn77299KksaNG6f3339fv//97zVv3jxflXlOrFaL8kcm6vVdFfqguFYzchJMrQcAgMHKrzrsbtq0SQUFBb32zZs3T5s2bTrtNW1tbXI6nb02X7lkVJIk6YPiWp/dAwAAnJlfhZfKykqlpqb22peamiqn06mWlpZTXrN8+XLFxcV5t6ysLJ/Vd2l3eCksbZCrrcNn9wEAAKfnV+HlfCxbtkwOh8O7lZaW+uxeWQmRykqIUIfH0JbD9T67DwAAOD2/Ci9paWmqqqrqta+qqkqxsbGKiIg45TV2u12xsbG9Nl+6ZGRX68v7vDoCAMAUfhVe8vPztXbt2l771qxZo/z8fJMqOhn9XgAAMJdPw0tTU5MKCwtVWFgoqWsodGFhoUpKSiR1vfJZuHCh9/zbb79dhw4d0g9/+EPt379ff/rTn/Tiiy/qrrvu8mWZfTJ7ZKIkaX9lo2qb2kyuBgCAwcen4WXbtm2aOnWqpk6dKkm6++67NXXqVN17772SpIqKCm+QkaScnBy98cYbWrNmjSZPnqzf/va3evzxx00fJv1pidF25abFSJI2HawzuRoAAAYfi2EYhtlF9Cen06m4uDg5HA6f9X/5+esf66/vH9Z1F2fpgQWTfHIPAAAGk778/farPi+BomfI9AcH6fcCAMBAI7ychxk5CQq1WlRa36KSumazywEAYFAhvJyHKHuopmTFS6L1BQCAgUZ4OU8MmQYAwByEl/PUE142HayTxxNUfZ4BAPBrhJfzNCUrXhFhIapzuVVU1Wh2OQAADBqEl/NkC7VqRk6CJF4dAQAwkAgvF+BS+r0AADDgCC8XYPaorqUCthyuV3unx+RqAAAYHAgvF2BcWqwSomxyuTv1UWmD2eUAADAoEF4ugNVqUf6IrtaX93l1BADAgCC8XKCeIdMbi1mkEQCAgUB4uUCXdPd72Vl6XK62DpOrAQAg+BFeLtCwhEgNjY9Qe6ehLUfqzS4HAICgR3i5QBaLxTtkeiP9XgAA8DnCSz/oGTL9Af1eAADwOcJLP5g9sqvl5eMKp+pdbpOrAQAguBFe+kFyjF1jU2MkSRsP8uoIAABfIrz0k0u8SwXw6ggAAF8ivPSTniHTtLwAAOBbhJd+MiMnQSFWi47WNau0vtnscgAACFqEl34SEx6mKVnxkmh9AQDAlwgv/eiSkQyZBgDA1wgv/Wh2z2R1B2tlGIbJ1QAAEJwIL/1o6rB4hYdZVdvk1oHqJrPLAQAgKBFe+pE9NMTb72X70ePmFgMAQJAivPSz6dkJkqRtRwgvAAD4AuGln+UNHyJJ2n6UFaYBAPAFwks/m5bVFV6O1DWrprHN5GoAAAg+hJd+FhcZpjGp0ZKkHSW8OgIAoL8RXnwgr7vfC512AQDof4QXH5ie3fXqaNsR+r0AANDfCC8+kNcdXvaUOdXa3mlyNQAABBfCiw9kJ0YqKdomd6dHu8scZpcDAEBQGZDw8sgjj2j48OEKDw/XzJkztWXLltOeu3LlSlksll5beHj4QJTZbywWi7f1hX4vAAD0L5+HlxdeeEF333237rvvPu3YsUOTJ0/WvHnzVF1dfdprYmNjVVFR4d2OHj3q6zL7HZPVAQDgGz4PL7/73e+0aNEi3Xzzzbrooou0YsUKRUZG6oknnjjtNRaLRWlpad4tNTXV12X2u57J6naUHGeRRgAA+pFPw4vb7db27dtVUFBw4oZWqwoKCrRp06bTXtfU1KTs7GxlZWXpqquu0t69e31Zpk+Mz4iVLdSqepdbh2pdZpcDAEDQ8Gl4qa2tVWdn50ktJ6mpqaqsrDzlNWPHjtUTTzyhf/zjH3r66afl8Xg0e/ZsHTt27JTnt7W1yel09tr8gT00RJMz4yTR7wUAgP7kd6ON8vPztXDhQk2ZMkWXXXaZXn75ZSUnJ+vRRx895fnLly9XXFycd8vKyhrgik/PO1kd/V4AAOg3Pg0vSUlJCgkJUVVVVa/9VVVVSktLO6efERYWpqlTp6q4uPiUx5ctWyaHw+HdSktLL7ju/uKdrI5FGgEA6Dc+DS82m015eXlau3atd5/H49HatWuVn59/Tj+js7NTu3fvVnp6+imP2+12xcbG9tr8xbTu8HKwxqXjLrfJ1QAAEBx8/tro7rvv1mOPPaannnpK+/bt0+LFi+VyuXTzzTdLkhYuXKhly5Z5z//Zz36mf//73zp06JB27NihG264QUePHtWtt97q61L7XUKUTSOSoyTR7wUAgP4S6usbXHvttaqpqdG9996ryspKTZkyRatXr/Z24i0pKZHVeiJDHT9+XIsWLVJlZaWGDBmivLw8bdy4URdddJGvS/WJ6dlDdKjGpe0lx1VwUeAN+QYAwN9YjCCbhMTpdCouLk4Oh8MvXiG9uLVUP/z7Ls0YnqAXbz+3V2UAAAw2ffn77XejjYJNz2R1Hx1rkLvDY3I1AAAEPsKLj41IitKQyDC1dXi0p5xFGgEAuFCEFx/rtUgj870AAHDBCC8DwDtZHSOOAAC4YISXATB9eM9kdSzSCADAhSK8DICJQ+MUFmJRbVObSuqbzS4HAICARngZAOFhIZowtGuRxm30ewEA4IIQXgZIzzpH20sILwAAXAjCywBhhWkAAPoH4WWA9AyX/qS6UY6WdpOrAQAgcBFeBkhyjF3ZiZEyDGkHr44AADhvhJcBxGR1AABcOMLLAJrOZHUAAFwwwssA6pmsrrC0Qe2dLNIIAMD5ILwMoFHJ0YoND1VLe6f2VTjNLgcAgIBEeBlAVqtF07r7vTBZHQAA54fwMsCYrA4AgAtDeBlgn56sjkUaAQDoO8LLAJucFacQq0WVzlaVNbSYXQ4AAAGH8DLAIm2hGp8RK4kh0wAAnA/CiwmmZMVLknYfc5hbCAAAAYjwYoKJQ+MkSbsILwAA9BnhxQSTu1te9pQ71Omh0y4AAH1BeDHByORoRdpC1Ozu1MGaJrPLAQAgoBBeTBBitWhCRtero49KG8wtBgCAAEN4McmkTPq9AABwPggvJpnYE17KCC8AAPQF4cUkkzPjJUn7yp1yd7DCNAAA54rwYpLsxEjFhofK3enRJ1WNZpcDAEDAILyYxGKxaFJ368tHxxpMrQUAgEBCeDGRt9NuKf1eAAA4V4QXE/W0vNBpFwCAc0d4MVFPy8snVY1qcXeaXA0AAIGB8GKi9LhwJUXb1ekx9HEFrS8AAJwLwouJujrtMlkdAAB9MSDh5ZFHHtHw4cMVHh6umTNnasuWLWc8/6WXXlJubq7Cw8M1ceJEvfnmmwNRpikILwAA9I3Pw8sLL7ygu+++W/fdd5927NihyZMna968eaqurj7l+Rs3btQ3v/lN3XLLLdq5c6euvvpqXX311dqzZ4+vSzXFZIZLAwDQJxbDMAxf3mDmzJm6+OKL9cc//lGS5PF4lJWVpe9///taunTpSedfe+21crlcev311737Zs2apSlTpmjFihVnvZ/T6VRcXJwcDodiY2P77z/ER2qb2jT9F29Lknb/9ArFhIeZXBEAAAOvL3+/fdry4na7tX37dhUUFJy4odWqgoICbdq06ZTXbNq0qdf5kjRv3rzTnt/W1ian09lrCyRJ0XYNjY+QJO1myDQAAGfl0/BSW1urzs5Opaam9tqfmpqqysrKU15TWVnZp/OXL1+uuLg475aVldU/xQ+gnn4vu+n3AgDAWQX8aKNly5bJ4XB4t9LSUrNL6rOJdNoFAOCchfryhyclJSkkJERVVVW99ldVVSktLe2U16SlpfXpfLvdLrvd3j8Fm4ROuwAAnDuftrzYbDbl5eVp7dq13n0ej0dr165Vfn7+Ka/Jz8/vdb4krVmz5rTnB4MJQ7taXo4db1G9y21yNQAA+Defvza6++679dhjj+mpp57Svn37tHjxYrlcLt18882SpIULF2rZsmXe83/wgx9o9erV+u1vf6v9+/frpz/9qbZt26Y77rjD16WaJi4iTDlJUZKkXbS+AABwRj59bSR1DX2uqanRvffeq8rKSk2ZMkWrV6/2dsotKSmR1XoiQ82ePVvPPvusfvzjH+u///u/NXr0aL366quaMGGCr0s11aTMOB2udWnXMYfmjE0xuxwAAPyWz+d5GWiBNs9Lj8ffO6RfvLFPBeNS9fiN080uBwCAAeU387zg3E3OipfEayMAAM6G8OInxmfEymqRqhvbVOVsNbscAAD8FuHFT0TaQjU6JUaS9FFpg7nFAADgxwgvfoQVpgEAODvCix+Z1NPvhTWOAAA4LcKLH5k0tKflpUFBNggMAIB+Q3jxI7npMQoLsaihuV2l9S1mlwMAgF8ivPgRe2iIxqV3jW3fVdZgbjEAAPgpwoufmTiUTrsAAJwJ4cXPeFeYZrg0AACnRHjxMxO7h0vvKXPI46HTLgAAn0V48TOjU6IVHmaVy92pQ7VNZpcDAIDfIbz4mdAQqyZk0O8FAIDTIbz4oYnMtAsAwGkRXvyQt9MuK0wDAHASwosf6lnj6ONyp9o7PSZXAwCAfyG8+KHhiVGKsYeqrcOjT6oazS4HAAC/QnjxQ1arhX4vAACcBuHFT01isjoAAE6J8OKnpg6LlyTtKDlubiEAAPgZwoufmjZsiCTpQHWTnK3tJlcDAID/ILz4qeQYu7ISImQYUmFJg9nlAADgNwgvfqyn9YVXRwAAnEB48WMnwkuDuYUAAOBHCC9+rCe87Cw5zgrTAAB0I7z4sdz0GIWHWdXY2qGDNawwDQCARHjxa2EhVu98Lzt5dQQAgCTCi9+j0y4AAL0RXvzcNCarAwCgF8KLn5uWzWR1AAB8GuHFzyVF2zUsIZLJ6gAA6EZ4CQCscwQAwAmElwDAZHUAAJxAeAkATFYHAMAJPg0v9fX1uv766xUbG6v4+Hjdcsstamo682Rrc+bMkcVi6bXdfvvtvizT7zFZHQAAJ/g0vFx//fXau3ev1qxZo9dff13vvvuubrvttrNet2jRIlVUVHi3Bx980Jdl+r1PT1ZHvxcAwGDns/Cyb98+rV69Wo8//rhmzpypSy+9VA8//LCef/55lZeXn/HayMhIpaWlebfY2FhflRkwvP1ejjaYWwgAACbzWXjZtGmT4uPjNX36dO++goICWa1Wbd68+YzXPvPMM0pKStKECRO0bNkyNTc3n/bctrY2OZ3OXlswYrI6AAC6hPrqB1dWViolJaX3zUJDlZCQoMrKytNe961vfUvZ2dnKyMjQrl279KMf/UhFRUV6+eWXT3n+8uXLdf/99/dr7f7o05PVOVraFRcRZnJFAACYo88tL0uXLj2pQ+1nt/379593QbfddpvmzZuniRMn6vrrr9eqVav0yiuv6ODBg6c8f9myZXI4HN6ttLT0vO/tz3omq5OkwtIGc4sBAMBEfW55ueeee3TTTTed8ZwRI0YoLS1N1dXVvfZ3dHSovr5eaWlp53y/mTNnSpKKi4s1cuTIk47b7XbZ7fZz/nmBbNqweJXUN2vH0eO6bEyy2eUAAGCKPoeX5ORkJSef/Q9nfn6+GhoatH37duXl5UmS1q1bJ4/H4w0k56KwsFCSlJ6e3tdSg8607CF6tbCcfi8AgEHNZx12x40bp/nz52vRokXasmWLPvjgA91xxx267rrrlJGRIUkqKytTbm6utmzZIkk6ePCgfv7zn2v79u06cuSI/vnPf2rhwoX6/Oc/r0mTJvmq1IDRM+KosLSByeoAAIOWT+d5eeaZZ5Sbm6u5c+fqi1/8oi699FL95S9/8R5vb29XUVGRdzSRzWbT22+/rSuuuEK5ubm65557tGDBAr322mu+LDNg5KbFKCIshMnqAACDms9GG0lSQkKCnn322dMeHz58uAzjRAtCVlaWNmzY4MuSAlpoiFWTMuO0+XC9dpQc1+jUGLNLAgBgwLG2UYDpGTLNZHUAgMGK8BJgTqwwTaddAMDgRHgJMFO7Z9rtmawOAIDBhvASYJKi7cpOZLI6AMDgRXgJQCcWaeTVEQBg8CG8BCAWaQQADGaElwA0lcnqAACDGOElAH16srpiJqsDAAwyhJcA1DNZnUS/FwDA4EN4CVDeyero9wIAGGQILwHqxGR1DeYWAgDAACO8BKieyeqKq5vkaGayOgDA4EF4CVCfnqxuZymvjgAAgwfhJYD1vDrayasjAMAgQngJYD2T1W1nxBEAYBAhvASwGTmJkrrCS3unx+RqAAAYGISXADY6JVoJUTa1tHdq1zGH2eUAADAgCC8BzGq1aGZOgiTpw0N1JlcDAMDAILwEuFkjul4dEV4AAIMF4SXA9YSXbUeOy91BvxcAQPAjvAS4T/d72V3WYHY5AAD4HOElwPXu91JvcjUAAPge4SUI0O8FADCYEF6CAP1eAACDCeElCNDvBQAwmBBeggD9XgAAgwnhJUjQ7wUAMFgQXoIE/V4AAIMF4SVI0O8FADBYEF6ChNVq0awR9HsBAAQ/wksQod8LAGAwILwEEfq9AAAGA8JLEPl0v5ddxxrMLgcAAJ8gvAQRi+XT/V54dQQACE6ElyBzot8LnXYBAMHJZ+Hll7/8pWbPnq3IyEjFx8ef0zWGYejee+9Venq6IiIiVFBQoAMHDviqxKDk7fdytJ5+LwCAoOSz8OJ2u3XNNddo8eLF53zNgw8+qIceekgrVqzQ5s2bFRUVpXnz5qm1tdVXZQadnn4vre0e+r0AAIKSz8LL/fffr7vuuksTJ048p/MNw9Af/vAH/fjHP9ZVV12lSZMmadWqVSovL9err77qqzKDDv1eAADBzm/6vBw+fFiVlZUqKCjw7ouLi9PMmTO1adOm017X1tYmp9PZaxvs6PcCAAhmfhNeKisrJUmpqam99qempnqPncry5csVFxfn3bKysnxaZyCg3wsAIJj1KbwsXbpUFovljNv+/ft9VespLVu2TA6Hw7uVlpYO6P39Ef1eAADBLLQvJ99zzz266aabznjOiBEjzquQtLQ0SVJVVZXS09O9+6uqqjRlypTTXme322W328/rnsGqp9/Lm7sr9eGhOk0fnmB2SQAA9Js+hZfk5GQlJyf7pJCcnBylpaVp7dq13rDidDq1efPmPo1YQpf8EYnd4aVed3zB7GoAAOg/PuvzUlJSosLCQpWUlKizs1OFhYUqLCxUU1OT95zc3Fy98sorkrpaC+6880794he/0D//+U/t3r1bCxcuVEZGhq6++mpflRm06PcCAAhWfWp56Yt7771XTz31lPf7qVOnSpLeeecdzZkzR5JUVFQkh8PhPeeHP/yhXC6XbrvtNjU0NOjSSy/V6tWrFR4e7qsyg9aolGglRtlU53Jr17EGXh0BAIKGxTAMw+wi+pPT6VRcXJwcDodiY2PNLsdUS57ZoTd2V+g/rxijO74w2uxyAAA4rb78/fabodLofycmq2O+FwBA8CC8BLFP93tp6+g0uRoAAPoH4SWI9fR76ZrvxXH2CwAACACElyDWNd9L91IBB1nnCAAQHAgvQc7b7+Uw4QUAEBwIL0Euf2RXy8uWw/UqqWs2uRoAAC4c4SXIjUqJ0edGJ6m909Cv/11kdjkAAFwwwssgsPTKXFks0msflbNQIwAg4BFeBoHxGXH6jylDJUn/35v7FGTzEgIABhnCyyBx9xVjZAu16sND9VpfVGN2OQAAnDfCyyCROSRSN88eLkl64F/71emh9QUAEJgIL4PI9+aMUlxEmIqqGvX3HcfMLgcAgPNCeBlE4iLDdMfloyRJv/v3J2pxs2QAACDwEF4GmW/nZ2tofIQqna164oPDZpcDAECfEV4GmfCwEP3XvLGSpBXrD6re5Ta5IgAA+obwMgh9dXKGxmfEqrGtQw+vO2B2OQAA9AnhZRCyWi1aduU4SdLTHx7V0TqXyRUBAHDuCC+D1KWjk/T5Mcldywa8xbIBAIDAQXgZxJbO71o24PVdFfqotMHscgAAOCeEl0HsooxY/cdUlg0AAAQWwssgd88VY2ULtWrz4Xq9U1RtdjkAAJwV4WWQGxofoZsvGS5JWv7mfjW7O8wtCACAsyC8QN+bM0rxkWE6UN2kbz62WXVNbWaXBADAaRFeoLiIMP31xosVHxmmj0obtODPGxk+DQDwW4QXSJLysofo74tnK3NIhI7UNetrf9rICCQAgF8ivMBrZHK0Xv7ebI3PiFWdy63r/vKh1u2vMrssAAB6Ibygl5SYcL3w3Xx9bnSSWto7tWjVdr2wtcTssgAA8CK84CTR9lA9cdPF+tq0oer0GPrR33frD29/wjwwAAC/QHjBKYWFWPXbaybrjstHSZL+8PYBLXt5tzo6PSZXBgAY7AgvOC2LxaL/nDdWv7h6gqwW6fmtpbrt/7bL1cZcMAAA8xBecFY3zMrWihvyZA+1at3+an19xSaVNbSYXRYAYJAivOCcXDE+Tc/dNktJ0Tbtq3Dqqj++r+1Hj5tdFgBgECK84JxNGzZEry65RLlpMaptcuubf/lQr+w8ZnZZAIBBhvCCPskcEqm/L56t/+eiVLk7PbrrhY/04Or98ngYiQQAGBg+Cy+//OUvNXv2bEVGRio+Pv6crrnppptksVh6bfPnz/dViThPUfZQPXpDnhbPGSlJ+tP6g1r8DB15AQADw2fhxe1265prrtHixYv7dN38+fNVUVHh3Z577jkfVYgLYbVa9KP5ufrdNybLFmLVW3urdM2KTSqnIy8AwMdCffWD77//fknSypUr+3Sd3W5XWlqaDyqCL3xtWqayEyP13f/bro8rnPrqHz/QXxbmadqwIWaXBgAIUn7X52X9+vVKSUnR2LFjtXjxYtXV1Z3x/La2Njmdzl4bBlZedsKnOvK26bq/fKh/7600uywAQJDyq/Ayf/58rVq1SmvXrtWvfvUrbdiwQVdeeaU6OztPe83y5csVFxfn3bKysgawYvTIHBKpvy2erYJxqXJ3eHTHszv17ic1ZpcFAAhCfQovS5cuPalD7We3/fv3n3cx1113nb761a9q4sSJuvrqq/X6669r69atWr9+/WmvWbZsmRwOh3crLS097/vjwkTbQ7Xihmn64sQ0uTs9uu3/tmnrkXqzywIABJk+9Xm55557dNNNN53xnBEjRlxIPSf9rKSkJBUXF2vu3LmnPMdut8tut/fbPXFhQkOs+sO1U9Xs3qb1RTX6zpNb9eyiWZqYGWd2aQCAINGn8JKcnKzk5GRf1XKSY8eOqa6uTunp6QN2T1w4W6hVK27I041PbNHmw/Va+MRmvfDdfI1JjTG7NABAEPBZn5eSkhIVFhaqpKREnZ2dKiwsVGFhoZqamrzn5Obm6pVXXpEkNTU16b/+67/04Ycf6siRI1q7dq2uuuoqjRo1SvPmzfNVmfCR8LAQ/fWmizU5K17Hm9t1/eObdaTWZXZZAIAg4LPwcu+992rq1Km677771NTUpKlTp2rq1Knatm2b95yioiI5HA5JUkhIiHbt2qWvfvWrGjNmjG655Rbl5eXpvffe47VQgIq2h+qpmy9WblqMahrbdP3jm5kHBgBwwSyGYQTVvO5Op1NxcXFyOByKjY01uxxIqmls0zce3aTDtS6NSIrSC9/NV3IMgRQAcEJf/n771VBpBKfkGLuevnWmhsZH6FCtS9/+62Y1NLvNLgsAEKAILxgQQ+Mj9MytM5UcY9f+ykbd+ORWNbEWEgDgPBBeMGCGJ0Xp6VtmKj4yTB+VNui2VdvU1nH6CQgBADgVwgsG1Ni0GK36zgxF2UK08WCd/vOlXfJ4gqrbFQDAxwgvGHCTMuO14tt5CrVa9NpH5Vr+r31mlwQACCCEF5jic6OT9eDXJ0mSHnvvsB5/75DJFQEAAgXhBab52rRM/Wh+riTpF2/s0z8/Kje5IgBAICC8wFS3XzZCN80eLkn6zxc/0saDteYWBADwe4QXmMpisegnX77IuxL1d1dt174Kp9llAQD8GOEFpguxWvS7b0zRjOEJamzr0E1PblEZywgAAE6D8AK/EB4WoscWTteY1GhVOdt04xNbmIUXAHBKhBf4jbjIMK28eYbSYsNVXN2kW5/aptZ2JrEDAPRGeIFfyYiP0FPfmaGY8FBtO3pc339up9wdHrPLAgD4EcIL/M7YtBg9tnC6bCFWrfm4Skue3cEyAgAAL8IL/NKsEYn6y8I82UK7Aszt/7edV0gAAEmEF/ixOWNT9MSNFys8zKp3imq0aBV9YAAAhBf4uUtHJ+nJm2YoIixE7x2o1XdWblWzu8PssgAAJiK8wO/lj0zUU59aifqmJ7fK1UaAAYDBivCCgDAjJ0GrbpmhaHuothyu141PbFFja7vZZQEATEB4QcDIy07Q07fO9A6j/vZft8jRQoABgMGG8IKAMiUrXs8tmqW4iDAVljbo23/dzEy8ADDIEF4QcCYMjdNzi2ZpSGSYdh1z6JuPbVZpfbPZZQEABgjhBQHpooxYPX9bvpKibdpX4dSXHnpP/95baXZZAIABQHhBwBqbFqNXl1yiKVnxcrZ26Lb/266fv/4xywkAQJAjvCCgZQ6J1Ivfzdetl+ZIkv76/mF949FNOnac10gAEKwILwh4tlCrfvzli/SXb+cpNjxUhaUN+tJD7+vtj6vMLg0A4AOEFwSNK8an6Y3/93OanBUvR0u7bl21Tb9842O1d/IaCQCCCeEFQSUrIVIvfeo10mPv8RoJAIIN4QVB57OvkXaWdL1GevKDw2rrYGFHAAh0hBcErc++Rrr/tY9V8LsNenVnmTwew+zyAADnyWIYRlD9K+50OhUXFyeHw6HY2Fizy4EfaO/06KVtx/SHtz9RdWObJGlceqx+NH+sLhuTLIvFYnKFAIC+/P0mvGDQaHZ36MkPjmjF+oNq7F6VetaIBC29cpymZMWbWxwADHKEF8ILzuC4y60/rS/WUxuPyt09EunKCWn6z3ljNTI52uTqAGBwIrwQXnAOyhpa9Ps1n+jvO47JMKQQq0UF41L0jelZumxMskJD6BIGAAOlL3+/ffav85EjR3TLLbcoJydHERERGjlypO677z653WdeAbi1tVVLlixRYmKioqOjtWDBAlVVMdkY+t/Q+Aj95prJWv2Dz6tgXIo6PYbe2lulW57apvwH1umBf+3XwZoms8sEAHyGz1peVq9erRdeeEHf/OY3NWrUKO3Zs0eLFi3St7/9bf3mN7857XWLFy/WG2+8oZUrVyouLk533HGHrFarPvjgg3O6Ly0vOF9FlY16aVupXt5ZpnrXiZA9PXuIvjE9S1+alK4oe6iJFQJA8PLb10a//vWv9ec//1mHDh065XGHw6Hk5GQ9++yz+vrXvy5J2r9/v8aNG6dNmzZp1qxZZ70H4QUXyt3h0br9VXpx2zGtL6pWz6jqSFuIvjwpXV+ZnKEZOQmyh4aYWygABJG+/P0e0P+NdDgcSkhIOO3x7du3q729XQUFBd59ubm5GjZs2DmHF+BC2UKtmj8hXfMnpKvK2aq/7ziml7Yd0+Fal17cdkwvbjumSFuIPjc6SV/ITdHlY1OUEhtudtkAMGgMWHgpLi7Www8/fMZXRpWVlbLZbIqPj++1PzU1VZWVlae8pq2tTW1tbd7vnU5nv9QLSFJqbLi+N2eUFl82UtuOHtfftx/T2v3Vqmls01t7q/TW3q7+WBOHxuny3BTNzU3RxKFxslqZOwYAfKXP4WXp0qX61a9+dcZz9u3bp9zcXO/3ZWVlmj9/vq655hotWrSo71WewfLly3X//ff3688EPstiseji4Qm6eHiCPB5De8udWre/Wuv2V+mjYw7tLuvaHlp7QEnRdn1+TJIuGZmkS0YlKS2OVhkA6E997vNSU1Ojurq6M54zYsQI2Ww2SVJ5ebnmzJmjWbNmaeXKlbJaTz/Aad26dZo7d66OHz/eq/UlOztbd955p+66666TrjlVy0tWVhZ9XjBgqhtbtb6oRu/sr9Z7B2rV1D0BXo8RyVHdQSZRs0YkKj7SZlKlAOC//KbDbllZmS6//HLl5eXp6aefVkjImTs49nTYfe6557RgwQJJUlFRkXJzc+mwi4Dg7vBo65F6vV9cq43Ftdpd5tCnl1GyWKTxGbHeVpkZOQkKD6PjLwD4RXgpKyvTnDlzlJ2draeeeqpXcElLS/OeM3fuXK1atUozZsyQ1DVU+s0339TKlSsVGxur73//+5KkjRs3ntN9CS/wJ46Wdn14qE6bDtbpg+JaHajuPW+MPdSqGTkJ+vzoZH1+TLLGpEaz1hKAQckvRhutWbNGxcXFKi4uVmZmZq9jPXmpvb1dRUVFam5u9h77/e9/L6vVqgULFqitrU3z5s3Tn/70J1+VCfhUXESY5o1P07zxXYG92tmqjd1B5v3iWlU4WvXegVq9d6BWv3xzn1Jj7frc6GR9bnSSPjc6WQlRvGICgM9ieQDAJIZhqLi6Se8eqNW7n9Ro8+E6tbZ7vMctlq5RTFdclKr5E9I0KiXGxGoBwLf84rWRWQgvCFSt7Z3aduS43jtQow2f1Gh/ZWOv4yOTozR/Qprmj0/XhKGxvF4CEFQIL4QXBIFqZ6vW7a/WW3sr9UFxnXcFbKlrXaYrxqdq/vg0TR+eoBDmlQEQ4AgvhBcEmcbWdr1TVKO39lTqnaJqNbs7vceSom368qQMfW3aUE0cGkeLDICARHghvCCItbZ36r0DtVq9p1Jv76uSo6Xde2xUSrS+Nm2orp4yVBnxESZWCQB9Q3ghvGCQaO/06IPiWr2ys0xv7a30dvi1WKTZIxP1tamZmj8hjdWwAfg9wgvhBYNQY2u7/rW7Un/fcUybD9d790eEhejKCWn6el6mZo1IZN0lAH6J8EJ4wSBXWt+sV3eW6eWdZTpc6/Luz06M1LUXZ+nreZlKiWHNJQD+g/BCeAEkdc0ls7O0QX/bfkz/LCz3rrsUarVo7rgUXTdjmD4/OpnRSgBMR3ghvAAnaXZ36PVdFXp+S4l2lDR49w+Nj9A10zP1jelZdPIFYBrCC+EFOKOiykY9v7VEL+8o845WslqkOWNTdN3FWfpCbopCQ06/AjwA9DfCC+EFOCet7Z16a2+lnt1c0quTb2qsXdfkZenai7OUlRBpYoUABgvCC+EF6LNDNU16YWup/rb9mOpcbkldQ64vHZWkb84YpoJxqbKF0hoDwDcIL4QX4Ly5Ozxa83GVnt9aovcO1Hr3J0XbtCAvU9ddPEw5SVEmVgggGBFeCC9Avyipa9YL20r04rZjqmls8+6/ePgQfT0vU1+alKFoJsAD0A8IL4QXoF+1d3q0bn+1nt9Sog2f1MjT/a8GE+AB6C+EF8IL4DNVzla9vKNMf9teqoM1JybAGxofoQXThmpBXqayE3mtBKBvCC+EF8DnDMNQYc8EeB+Vq7G1w3vs4uFD9NUpQ3XlhDQlRdtNrBJAoCC8EF6AAdXa3qk1H1fpb9uP6b0DJ14rWS3S7JFJ+vKkdM2fkKb4SJu5hQLwW4QXwgtgmkpHq177qFyv7yrXR8cc3v2hVosuHZ2kL0/K0BXjUxUbHmZilQD8DeGF8AL4haN1Lr2+q0Kv76rQvgqnd78txKrLxibriotSdXluCq+WABBeCC+A/zlY06TXP6rQa7vKVVzd5N1vsUhTsuJVMC5VX8hNUW5ajCwWRi0Bgw3hhfAC+C3DMFRU1ag3d1dq3f4q7Slz9jo+ND5CX8hN0RfGpSh/RKLCw0JMqhTAQCK8EF6AgFHpaNW6/dVat79K7xfXqrXd4z0WERaiWSMSNHtkkvJHJmpceqxCmEsGCEqEF8ILEJBa3J3adKhWb++r1rp91ap0tvY6HhcR5g0zs0cmalRKNK+YgCBBeCG8AAHPMAztq2jUxoO12nSwTpsP16upraPXOUnRduWPTNSMnARNGxavsakxCg1h8UggEBFeCC9A0Ono9Gh3mUMbD9Zp08E6bT1Sr7YOT69zIm0hmpQZp6nDhmhqVrymDhui5BhGMgGBgPBCeAGCXltHpwpLGrTxYJ12lBxXYUmDGj/TMiNJWQkRmpo1RFOy4jU+I1bjMmKZYwbwQ4QXwgsw6HR6DB2sadLOkuPacbRBO0uP60B1k071L9ywhEiNz4jV+IxYXZQRq/EZcUqJsdN/BjAR4YXwAkCSs7Vdu0od2lFyXLuOOfRxuUPljtZTnpsUbdO49FiNTY3RmNQYjUmL0eiUaEXZQwe4amBwIrwQXgCcxnGXWx9XOLW33KGPy53aW+7UwZom73pMn5U5JKIrzKTGaExqtMakxmhkcrQibMw/A/QnwgvhBUAftLg7VVTVqI/LnfqkqlEHqhv1SVWTahrbTnvN0PgIjUyJ1sjkKI1MjtbI5GiNSolWUrSN10/AeSC8EF4A9IPjLrc+qWrs3pq8n483t5/2mtjwUI1MidaIpGiNSI7SiKQo5SRHaXhiFLMFA2dAeCG8APChepdbB2uadLC6qetrjUsHa5pUWt982tdPFouUERfhDTQjkqOVkxSlnKQoZcRHMHMwBj3CC+EFgAla2zt1pM6lg9UuHa5t0qEalw7VunSopknO1pOHcfewhViVnRjZFWaSo5STGOX9nBzNKCgMDn35+003egDoJ+FhIcpNi1VuWu9/eA3DUL3LrUO1Lh2ucelgd7A5UuvS0bpmuTs9OlDdpAOfWm27R7Q9VMOTIpWTFK2cxEjvK6gRSdGKi2S+GgxOPmt5OXLkiH7+859r3bp1qqysVEZGhm644Qb9z//8j2w222mvmzNnjjZs2NBr33e/+12tWLHinO5LywuAQNLpMVTe0KLDtS7vdqi2K9gcO37611CSlBBl0/DE7mDTHXC6gk6UIm38vykCi1+0vOzfv18ej0ePPvqoRo0apT179mjRokVyuVz6zW9+c8ZrFy1apJ/97Gfe7yMjI31VJgCYKsRqUVZCpLISIvX5Mcm9jrV1dKqkrlmHa106UtcdbGq6Plc521Tvcqve5daOkoaTfm5qrN3bpyYnqau1JicpSsMSI2UPpeMwApvPwsv8+fM1f/587/cjRoxQUVGR/vznP581vERGRiotLc1XpQFAQLCHhmh0aoxGp8acdMzV1uENNIdrXDrc/flIrUvHm9tV5WxTlbNNHx6q73VdT8fhnKQoDU+K9IaanKQoZSVEKoyFLREABrRd0eFwKCEh4aznPfPMM3r66aeVlpamr3zlK/rJT35y2taXtrY2tbWdmIvB6XT2W70A4K+i7KEanxGn8RlxJx1raHZ7X0EdqXXpcF2zDtc26Uhts5raOlTW0KKyhha9X9z7uhCrRUPjIzQ8KUrDE7uCTU/AyRwSKVsowQb+YcDCS3FxsR5++OGztrp861vfUnZ2tjIyMrRr1y796Ec/UlFRkV5++eVTnr98+XLdf//9vigZAAJSfKRNU4fZNHXYkF77DcNQbZPb22JzxPs6qllHal1qae9USX2zSuqb9e5nfqbVIg0dEtEVaBKjlJ0YqWEJkRqeFKVhCZHMYYMB1ecOu0uXLtWvfvWrM56zb98+5ebmer8vKyvTZZddpjlz5ujxxx/vU4Hr1q3T3LlzVVxcrJEjR550/FQtL1lZWXTYBYA+MAxDVc42Halz6Wh3oDla59KRuq6vze7OM16fGmtXdmKUshMilZ0YqezErlAzLCFS8ZFhDPfGWfl0npeamhrV1dWd8ZwRI0Z4RxSVl5drzpw5mjVrllauXCmrtW/Nji6XS9HR0Vq9erXmzZt31vMZbQQA/cswDNU0tnk7Dh+ta9bR+q5Qc7SuWY1nmMNGkmLCQ71BZlhCpIYlnvicER9BPxtI8vFoo+TkZCUnJ5/9RHW1uFx++eXKy8vTk08+2efgIkmFhYWSpPT09D5fCwC4cBaLRSmx4UqJDdfMEYm9jhmGoYbmdh2pc6mkvllHapt1tN6lkrqu10/VjW1qbO3Q3u5FMD/LapHSYsOVmRCprCGRykqI6P4aqcwhEUqNDWf2YZzEZ/O8lJWVac6cOcrOztZTTz2lkJAT70N7RhKVlZVp7ty5WrVqlWbMmKGDBw/q2Wef1Re/+EUlJiZq165duuuuu5SZmXnS3C+nQ8sLAPiPFnenjh1v1tHuMFNS36zS+hOf2zo8Z7zeFmJVeny4hsZHKHNIhIbGR2rokJ7PEUqPC1coLTdBwS/meVmzZo2Ki4tVXFyszMzMXsd68lJ7e7uKiorU3NwsSbLZbHr77bf1hz/8QS6XS1lZWVqwYIF+/OMf+6pMAIAPRdhOP9zbMAzVNLWptL5Fx453hZpjx1tUerxZpfUtKm9okbvT0/Waqq75lD/fapHS4yKUER+ujPgIpcdFaGh8ePe+roATGxFKn5sgw9pGAAC/1NHpUaWzVWXHu4Z2lx1v0bGez92b+ywtN5IUaQvpDjbhSo8LV1pcRPfXru/TYwk4/sAvWl4AALgQoSFWZQ6JVOaQU8/z5fEYqm1q07GGrlaaioZWlfV8drSqvKFFdS63mt2dKq5uUvEp1o7qEREW4g00qbE9m73X55SYcOa68ROEFwBAQLJaT3QknvaZOW16tLZ3eoNMhaNVlY6er61dX52tqne51dLe2bUCeK3rjPdMjLJ13TPG3rV1h5rknu9jwpUSa2feGx8jvAAAglZ4WIh3+YPTaW3vVJWz1RtqKp2tqvJubap0tKq6sVXtnYbqXG7VudzaV3Hm+8bYQ5UcY1dSjF3J0XYlRdu6vo/u2nqOJUbZCDrngfACABjUwsNCuibYSzx9wDEMQ/Uud9eaUY2tqnG2qbqxVdWNbappbFN1Y/f3zja1dXjU2NahxraOs7bkSF1BJzHapsTorjCT2B12ej4nRtk0JMrm/cq8OIQXAADOymKxdAWJaLsu0uk7kxqGoca2DlU721Tb1BVsaps+/dnda397p+ENOkdOM6Lqs2LCQ3sHmkibEqK7v0baFB8ZpiHd+4dEhik+0hZ0c+UQXgAA6CcWi0Wx4WGKDQ/TqJToM55rGIacrR2qa2rreh3V1BVu6prcqnO1qa476NS73Kp3uXW82S2PITW2dqix9dzDjsUixYaHKSHKpriIMG+g6frcFXbiu/fFR3R9josIU0x4mN+GHsILAAAmsFgsiovoCgojzmHieo/HkKOlXXXdQaau6USoqWtyq6G563N9c7samruONbZ2yDAkR0u7HC3tfayv65VWT9CJjwxTbESY4iPCNHRIhL43Z9R5/pdfOMILAAABwGq1dL0OirKd8zUdnR41tLTruMut492hpqG5XQ0tXV+PN7fL8anPDc1uOVra1ezulGFIztYOOU+xdtWI5CjCCwAA6H+hIVbvCKe+cHd4ultrusJMQ3N7r68x4ebGB8ILAADoxRZqVXJM15Buf8R4KwAAEFAILwAAIKAQXgAAQEAhvAAAgIBCeAEAAAGF8AIAAAIK4QUAAAQUwgsAAAgohBcAABBQCC8AACCgEF4AAEBAIbwAAICAQngBAAABJehWlTYMQ5LkdDpNrgQAAJyrnr/bPX/HzyTowktjY6MkKSsry+RKAABAXzU2NiouLu6M51iMc4k4AcTj8ai8vFwxMTGyWCz9+rOdTqeysrJUWlqq2NjYfv3ZOBnPe2DxvAcWz3tg8bwH1vk8b8Mw1NjYqIyMDFmtZ+7VEnQtL1arVZmZmT69R2xsLL/8A4jnPbB43gOL5z2weN4Dq6/P+2wtLj3osAsAAAIK4QUAAAQUwksf2O123XfffbLb7WaXMijwvAcWz3tg8bwHFs97YPn6eQddh10AABDcaHkBAAABhfACAAACCuEFAAAEFMILAAAIKISXc/TII49o+PDhCg8P18yZM7VlyxazSwoa7777rr7yla8oIyNDFotFr776aq/jhmHo3nvvVXp6uiIiIlRQUKADBw6YU2yAW758uS6++GLFxMQoJSVFV199tYqKinqd09raqiVLligxMVHR0dFasGCBqqqqTKo4sP35z3/WpEmTvBN15efn61//+pf3OM/atx544AFZLBbdeeed3n088/7z05/+VBaLpdeWm5vrPe7LZ014OQcvvPCC7r77bt13333asWOHJk+erHnz5qm6utrs0oKCy+XS5MmT9cgjj5zy+IMPPqiHHnpIK1as0ObNmxUVFaV58+aptbV1gCsNfBs2bNCSJUv04Ycfas2aNWpvb9cVV1whl8vlPeeuu+7Sa6+9ppdeekkbNmxQeXm5vva1r5lYdeDKzMzUAw88oO3bt2vbtm36whe+oKuuukp79+6VxLP2pa1bt+rRRx/VpEmTeu3nmfev8ePHq6Kiwru9//773mM+fdYGzmrGjBnGkiVLvN93dnYaGRkZxvLly02sKjhJMl555RXv9x6Px0hLSzN+/etfe/c1NDQYdrvdeO6550yoMLhUV1cbkowNGzYYhtH1bMPCwoyXXnrJe86+ffsMScamTZvMKjOoDBkyxHj88cd51j7U2NhojB492lizZo1x2WWXGT/4wQ8Mw+D3u7/dd999xuTJk095zNfPmpaXs3C73dq+fbsKCgq8+6xWqwoKCrRp0yYTKxscDh8+rMrKyl7PPy4uTjNnzuT59wOHwyFJSkhIkCRt375d7e3tvZ53bm6uhg0bxvO+QJ2dnXr++eflcrmUn5/Ps/ahJUuW6Etf+lKvZyvx++0LBw4cUEZGhkaMGKHrr79eJSUlknz/rINuYcb+Vltbq87OTqWmpvban5qaqv3795tU1eBRWVkpSad8/j3HcH48Ho/uvPNOXXLJJZowYYKkrudts9kUHx/f61ye9/nbvXu38vPz1draqujoaL3yyiu66KKLVFhYyLP2geeff147duzQ1q1bTzrG73f/mjlzplauXKmxY8eqoqJC999/vz73uc9pz549Pn/WhBdgkFqyZIn27NnT6x01+t/YsWNVWFgoh8Ohv/3tb7rxxhu1YcMGs8sKSqWlpfrBD36gNWvWKDw83Oxygt6VV17p/Txp0iTNnDlT2dnZevHFFxUREeHTe/Pa6CySkpIUEhJyUg/pqqoqpaWlmVTV4NHzjHn+/euOO+7Q66+/rnfeeUeZmZne/WlpaXK73WpoaOh1Ps/7/NlsNo0aNUp5eXlavny5Jk+erP/93//lWfvA9u3bVV1drWnTpik0NFShoaHasGGDHnroIYWGhio1NZVn7kPx8fEaM2aMiouLff77TXg5C5vNpry8PK1du9a7z+PxaO3atcrPzzexssEhJydHaWlpvZ6/0+nU5s2bef7nwTAM3XHHHXrllVe0bt065eTk9Dqel5ensLCwXs+7qKhIJSUlPO9+4vF41NbWxrP2gblz52r37t0qLCz0btOnT9f111/v/cwz952mpiYdPHhQ6enpvv/9vuAuv4PA888/b9jtdmPlypXGxx9/bNx2221GfHy8UVlZaXZpQaGxsdHYuXOnsXPnTkOS8bvf/c7YuXOncfToUcMwDOOBBx4w4uPjjX/84x/Grl27jKuuusrIyckxWlpaTK488CxevNiIi4sz1q9fb1RUVHi35uZm7zm33367MWzYMGPdunXGtm3bjPz8fCM/P9/EqgPX0qVLjQ0bNhiHDx82du3aZSxdutSwWCzGv//9b8MweNYD4dOjjQyDZ96f7rnnHmP9+vXG4cOHjQ8++MAoKCgwkpKSjOrqasMwfPusCS/n6OGHHzaGDRtm2Gw2Y8aMGcaHH35odklB45133jEknbTdeOONhmF0DZf+yU9+YqSmphp2u92YO3euUVRUZG7RAepUz1mS8eSTT3rPaWlpMb73ve8ZQ4YMMSIjI43/+I//MCoqKswrOoB95zvfMbKzsw2bzWYkJycbc+fO9QYXw+BZD4TPhheeef+59tprjfT0dMNmsxlDhw41rr32WqO4uNh73JfP2mIYhnHh7TcAAAADgz4vAAAgoBBeAABAQCG8AACAgEJ4AQAAAYXwAgAAAgrhBQAABBTCCwAACCiEFwAAEFAILwAAIKAQXgAAQEAhvAAAgIBCeAEAAAHl/wdPaTxj5GzITAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" training loop\n",
    "(Jordan et al. 2024) URL: https://github.com/KellerJordan/modded-nanogpt\n",
    "124M 10x speedup: 45.0m -> 04.4m\n",
    "========================================================================\n",
    "- network architecture: rotary embeddings, QK-norm, RelU^2\n",
    "- muon optimizer\n",
    "- untie head & embedding, FP8 matmul for head, softcap logits (gemma 2)\n",
    "- projection and classification layers init to zero (muP)\n",
    "- skip connections from embedding to every block (and between) via U-net\n",
    "- flexattention with long-short sliding window attention (gemma 2), window size warmup\n",
    "\n",
    "        124M history:\n",
    "        01. 45.0m baseline\n",
    "        02. 31.4m tuned lr, rotary embeddings\n",
    "        03. 24.9m muon optimizer\n",
    "        04. 22.3m muon improvements\n",
    "        05. 15.2m pad embeddings, ReLU^2, zero init, QK-norm\n",
    "        06. 13.1m muon overhead\n",
    "        07. 12.0m pytorch 2.5.0\n",
    "        08. 10.8m united embedding and head\n",
    "        09. 08.2m value and embed skip connections, momentum warmup, logit softcap\n",
    "        10. 07.8m bfloat16 act\n",
    "        11. 07.2m u-net pattern skip cojnnections, double lr\n",
    "        12. 05.0m 1024-ctx dense causal attn -> 64K-ctx flex attention\n",
    "        13. 04.6m attention window warmup\n",
    "        14. 04.4m value embededdings\n",
    "\"\"\"\n",
    "\n",
    "# 1. dataloader\n",
    "import torch\n",
    "\n",
    "# tokenize\n",
    "B, T = 4, 8\n",
    "with open('./data/shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "text = text[:1000]\n",
    "# vocab = sorted(list(set(''.join(text))))\n",
    "# V = len(vocab)\n",
    "import tiktoken\n",
    "encoder = tiktoken.get_encoding('gpt2')\n",
    "tokens = encoder.encode(text)\n",
    "data = torch.tensor(tokens[:(B*T + 1)])\n",
    "X_BT, Y_BT = data[:-1].view(B, T), data[1:].view(B, T)\n",
    "X_BT, Y_BT = X_BT.to(device), Y_BT.to(device)\n",
    "\n",
    "print(X_BT)\n",
    "print(Y_BT)\n",
    "for b in range(B):\n",
    "    print('batch', b)\n",
    "    for t in range(T):\n",
    "        context = X_BT[b, :t+1]\n",
    "        target = Y_BT[b, t]\n",
    "        print('x:', context, '->', 'y:', target)\n",
    "\n",
    "print(\"==========================================\")\n",
    "\n",
    "# 2. training loop\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "steps, losses = [], []\n",
    "for step in range(50):\n",
    "    # 1. forward\n",
    "    optimizer.zero_grad()\n",
    "    logits_BTV, loss = model(X_BT, Y_BT)\n",
    "    # 2. backward\n",
    "    loss.backward()\n",
    "    # 3. step\n",
    "    optimizer.step()\n",
    "\n",
    "    steps.append(step)\n",
    "    losses.append(loss.log10().item())\n",
    "    print(f\"step: {step}, loss: {loss.item()}\")\n",
    "\n",
    "plt.plot(steps, losses)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Hello, I'm a language model, speak.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> Hello, I'm a language model, speak.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> Hello, I'm a language model, speak.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> Hello, I'm a language model, speak.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> Hello, I'm a language model, speak.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "B, T_MAX = 5, 30\n",
    "model.eval()\n",
    "\n",
    "import tiktoken\n",
    "encoder = tiktoken.get_encoding('gpt2')\n",
    "tokens = encoder.encode(\"Hello, I'm a language model,\")\n",
    "tokens_T = torch.tensor(tokens, dtype=torch.long) # # (T,)\n",
    "tokens_BT = tokens_T.unsqueeze(0).repeat(5, 1) # (B,T)\n",
    "X_BT = tokens_BT.to(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "while X_BT.size(1) < T_MAX:\n",
    "    with torch.no_grad():\n",
    "        logits_BTV, _ = model(X_BT)\n",
    "        logits_BV = logits_BTV[:, -1, :]\n",
    "        probs_ = F.softmax(logits_BV, dim=-1)\n",
    "        topk_probs_, topk_indices_ = torch.topk(probs_, 50, dim=-1)\n",
    "\n",
    "        X_B1 = torch.gather(topk_indices_, -1, torch.multinomial(topk_probs_, 1))\n",
    "        X_BT = torch.cat((X_BT, X_B1), dim=1)\n",
    "\n",
    "for b in range(B):\n",
    "    tokens = X_BT[b, :T_MAX].tolist()\n",
    "    decoded = encoder.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
